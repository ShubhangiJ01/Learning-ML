{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2: Classification\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://www.continuum.io/downloads). Then save this file to your computer (use \"Raw\" link on gist\\github), run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/).\n",
    "\n",
    "To run code in a cell or to render [Markdown](https://en.wikipedia.org/wiki/Markdown)+[LaTeX](https://en.wikipedia.org/wiki/LaTeX) press `Ctr+Enter` or `[>|]`(like \"play\") button above. To edit any code or text cell [double]click on its content. To change cell type, choose \"Markdown\" or \"Code\" in the drop-down menu above.\n",
    "\n",
    "If a certain output is given for some cells, that means that you are expected to get similar results in order to receive full points (small deviations are fine). For some parts we have already written the code for you. You should read it closely and understand what it does.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression\n",
    "\n",
    "In this part of the exercise, you will build a logistic regression model to predict whether a student\n",
    "gets admitted into a university.\n",
    "\n",
    "Suppose that you are the administrator of a university department and you want to determine\n",
    "each applicant’s chance of admission based on their results on two exams. You have historical\n",
    "data from previous applicants in *ex2data1.txt* that you can use as a training set for logistic regression. For each\n",
    "training example, you have the applicant’s scores on two exams and the admissions decision.\n",
    "\n",
    "Your task is to build a classification model that estimates an applicant’s probability of admission based on the scores from those two exams. This outline and code framework will guide you through the exercise.\n",
    "\n",
    "**1\\.1 Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested with:\n",
      "Python 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n",
      "{'numpy': '1.18.1', 'matplotlib': '3.1.3'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print('Tested with:')\n",
    "print('Python', sys.version)\n",
    "print({x.__name__: x.__version__ for x in [np, matplotlib]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.1 Visualizing the data**\n",
    "\n",
    "Before starting to implement any learning algorithm, it is always good to visualize the data if possible. This first part of the code will load the data and display it on a 2-dimensional plot by calling the function plotData. The axes are the two exam scores, and the positive and negative examples are shown with different markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Try to fit your code and comments into 80 charecters because\n",
    "# - it is guaranteed to look as intened on any screen size\n",
    "# - it encourages you to write \"flater\" logic that is easier to reason about\n",
    "# - it encourages you to decompose logic into comprehansible blocks.\n",
    "# \n",
    "# Try to avoid reassinging/mutating variables because when you encounter an \n",
    "# unexplainable bugs (you will) it is easier to have the whole history\n",
    "# of values to reason about.\n",
    "#\n",
    "# Using %debug magic to run pdb might be useful for debugging. \n",
    "# It is just like gdb but for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) 30.05882244669796 99.82785779692128 float64\n",
      "(100, 1) 0.0 1.0 float64\n",
      "(100, 3) 1.0 99.82785779692128 float64\n",
      "(100, 1) 0 1 int32\n"
     ]
    }
   ],
   "source": [
    "# it is good to isolate logical parts to avoid variables leaking into the\n",
    "# global scope and messing up your logic later in weird ways\n",
    "\n",
    "def read_classification_csv_data(fn, add_ones=False):\n",
    "    # read comma separated data\n",
    "    data = np.loadtxt(fn, delimiter=',')\n",
    "    X_, y_ = data[:, :-1], data[:, -1, None]  # a fast way to keep last dim\n",
    "\n",
    "    # printing statistics of data before working with it might have saved\n",
    "    # hundreds hours of of my time, do not repeat my errors :)\n",
    "    print(X_.shape, X_.min(), X_.max(), X_.dtype)\n",
    "    print(y_.shape, y_.min(), y_.max(), y_.dtype)\n",
    "    # aha, y is float! this is not what we expected\n",
    "    # what might go wrong with further y == 0 checks?\n",
    "    # A: floating point equality comparison, that's what!\n",
    "\n",
    "    # insert the column of 1's into the \"X\" matrix (for bias)\n",
    "    X = np.insert(X_, X_.shape[1], 1, axis=1) if add_ones else X_\n",
    "    y = y_.astype(np.int32)\n",
    "    return X, y\n",
    "    \n",
    "X_data, y_data = read_classification_csv_data('ex2data1.txt', add_ones=True)\n",
    "print(X_data.shape, X_data.min(), X_data.max(), X_data.dtype)\n",
    "print(y_data.shape, y_data.min(), y_data.max(), y_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# how does the *X[y.ravel()==1, :2].T trick work?\n",
    "# https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists\n",
    "\n",
    "def plot_data(X, y, labels, markers, xlabel, ylabel, figsize=(10, 6), ax=None):\n",
    "    if figsize is not None:\n",
    "        plt.figure(figsize=figsize)\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "    for label_id, (label, marker) in enumerate(zip(labels, markers)):        \n",
    "        ax.plot(*X[y.ravel()==label_id, :2].T, marker, label=label)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "student_plotting_spec = {\n",
    "    'X': X_data,\n",
    "    'y': y_data, \n",
    "    'xlabel': 'Exam 1 score', \n",
    "    'ylabel': 'Exam 2 score',\n",
    "    'labels': ['Not admitted', 'Admitted'], \n",
    "    'markers': ['yo', 'k+'], \n",
    "    'figsize': (10, 6)\n",
    "}\n",
    "    \n",
    "plot_data(**student_plotting_spec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.2  Sigmoid function** \n",
    "\n",
    "Before you start with the actual cost function, recall that the logistic regression hypothesis is defined as:\n",
    "\n",
    "$h_\\theta(x) = g(\\theta^Tx)$\n",
    "\n",
    "where function g is the sigmoid function. The sigmoid function is defined as:\n",
    "\n",
    "$g(z) = \\dfrac{1}{1+e^{-z}}$\n",
    "\n",
    "Your first step is to implement/find a sigmoid function so it can be called by the rest of your program. Your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid function on every element. \n",
    "\n",
    "When you are finished, (a) plot the sigmoid function, and (b) test the function with a scalar, a vector, and a matrix. For scalar large positive values of x, the sigmoid should be close to 1, while for scalar large negative values, the sigmoid should be close to 0. Evaluating sigmoid(0) should give you exactly 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-dcb2d1e9925f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mcheck_that_sigmoid_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-dcb2d1e9925f>\u001b[0m in \u001b[0;36mcheck_that_sigmoid_f\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# what might do wrong with float precision?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0msigm_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigm_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sigmoid function\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# check out scipy.special for great variaty of vectorized functions\n",
    "# remember that sigmoid is the inverse of logit function\n",
    "# maybe worth checking out scipy.special.logit first\n",
    "\n",
    "sigmoid = None\n",
    "\n",
    "def check_that_sigmoid_f(f):\n",
    "    # don't use np.arange with float step because it works as\n",
    "    # val_{i+1} = val_i + step while val_i < end\n",
    "    # what might do wrong with float precision?\n",
    "    x_test = np.linspace(-10, 10, 50)\n",
    "    sigm_test = f(x_test)\n",
    "    plt.plot(x_test, sigm_test)\n",
    "    plt.title(\"Sigmoid function\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # why should analytical_diff almost== finite_diff for sigmoid?\n",
    "    analytical_diff = sigm_test*(1-sigm_test)\n",
    "    finite_step = x_test[1]-x_test[0]\n",
    "    finite_diff = np.diff(sigm_test) / finite_step\n",
    "    print(x_test.shape, finite_diff.shape)\n",
    "    plt.plot(x_test[:-1]+finite_step/2, finite_diff)\n",
    "    plt.plot(x_test, analytical_diff)\n",
    "    plt.title(\"Numerical (finite difference) derivative of 1d sigmoid\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "check_that_sigmoid_f(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.3  Cost function and gradient**\n",
    "\n",
    "Now you will implement the cost function and gradient for logistic regression. Complete the code\n",
    "in the functions *hyposesis_function* and *binary_logistic_loss* below to return the value of the hypothesis function and the cost, respectively. Recall that the cost function in logistic regression is\n",
    "\n",
    "$j(\\theta) \\ = \\ \\frac{1}{m} \\ \\sum_{i=1}^{m} \\ [ \\ -y^{(i)} log(h_\\theta(x^{(i)})) \\ - \\ (1 - y^{(i)})log(1-h_\\theta(x^{(i)})) \\ ]$\n",
    "\n",
    "and the gradient of the cost is a vector of the same length as $\\theta$ where the $j^{th}$ element (for $j = 0, 1,...,n$) is defined as follows:\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}} \\ = \\ \\frac{1}{m} \\ \\sum_{i=1}^{m} \\ (h_\\theta(x^{(i)})-y^{(i)}) x_j^{(i)}$\n",
    "\n",
    "where $m$ is the number of points and $n$ is the number of features. Note that while this gradient looks identical to the linear regression gradient, the formula is\n",
    "actually different because linear and logistic regression have different definitions of $h_\\theta(x)$.\n",
    "\n",
    "What should be the value of the loss for $\\theta = \\bar 0$ regardless of input? Why? Make sure your code also outputs this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are trying to fit a function that would return a \n",
    "# \"probability of \"\n",
    "\n",
    "# hyposesis_function describes parametric family of functions that we are\n",
    "# going to pick our \"best fitting function\" from. It is parameterized by\n",
    "# real-valued vector theta, i.e. we are going to pick\n",
    "#    h_best = argmin_{h \\in H} logistic_loss_h(x, y, h)\n",
    "# but because there exist a bijection between theta's and h's it is \n",
    "# eqvivalent to choosing\n",
    "#    theta_best = argmin_{theta \\in H} logistic_loss_theta(x, y, theta)\n",
    "\n",
    "def hyposesis_function(x, theta):\n",
    "    raise NotImplementedError('Implement it yourself.')\n",
    "\n",
    "# negative log likelihood of observing sequence of integer\n",
    "# y's given probabilities y_pred's of each Bernoulli trial\n",
    "# recommentation: convert both variables to float's \n",
    "# or weird sign stuff might happen like -1*y != -y for uint8\n",
    "# use np.mean and broadcasting\n",
    "def binary_logistic_loss(y, y_pred):\n",
    "    assert y_pred.shape == y.shape\n",
    "    # or weird sign stuff happens! like -1*y != -y\n",
    "    y, y_pred = y.astype(np.float64), y_pred.astype(np.float64)\n",
    "    # When y_pred = 0, log(0) = -inf, \n",
    "    # we could add a small constant to avoid this case\n",
    "    CONSTANT = 0.000001\n",
    "    y_pred = np.clip(y_pred, 0+CONSTANT, 1-CONSTANT)\n",
    "    \n",
    "    #TODO: Calculate the log likelihoods\n",
    "    raise NotImplementedError('Implement it yourself.')\n",
    "    \n",
    "\n",
    "def logistic_loss_theta_grad(x, y, h, theta):\n",
    "    y_pred = h(x, theta)\n",
    "    point_wise_grads = (y_pred - y)*x\n",
    "    grad = np.mean(point_wise_grads, axis=0)[:, None]\n",
    "    assert grad.shape == theta.shape\n",
    "    return grad\n",
    "    \n",
    "\n",
    "def logistic_loss_theta(x, y, h, theta):\n",
    "    return binary_logistic_loss(y, h(x, theta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that with theta as zeros, cost is about 0.693:\n",
    "theta_init = np.zeros((X_data.shape[1], 1))\n",
    "print(logistic_loss_theta(X_data, y_data, hyposesis_function, theta_init))\n",
    "print(logistic_loss_theta_grad(X_data, y_data, hyposesis_function, theta_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.4 Learning parameters using *scipy.optimize***\n",
    "\n",
    "In the previous assignment, you found the optimal parameters of a linear regression model by\n",
    "implementing gradient descent. You wrote a cost function and calculated its gradient, then took\n",
    "a gradient descent step accordingly. This time, instead of taking gradient descent steps, you will\n",
    "use a scipy.optimize built-in function called *scipy.optimize.minimize*. In this case, we will use\n",
    "the *[conjugate gradient algorithm](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cg.html)*. \n",
    "\n",
    "The final $\\theta$ value will then be used to plot the\n",
    "decision boundary on the training data, as seen in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(theta_init, loss, loss_grad, max_iter=10000, print_every=1000, optimizer_fn=None, show=False):\n",
    "    theta = theta_init.copy()\n",
    "    opt_args = {'x0': theta_init, 'fun': loss, 'jac': loss_grad, 'options': {'maxiter': max_iter}}\n",
    "\n",
    "    loss_curve = []\n",
    "    def scipy_callback(theta):\n",
    "        f_value = loss(theta)\n",
    "        loss_curve.append(f_value)\n",
    "        \n",
    "    if optimizer_fn is None:\n",
    "        optimizer_fn = partial(scipy.optimize.minimize, method='CG', callback=scipy_callback)\n",
    "\n",
    "    opt_result = optimizer_fn(**opt_args)\n",
    "    \n",
    "    if show:\n",
    "        plt.plot(loss_curve)\n",
    "        plt.show()\n",
    "    \n",
    "    return opt_result['x'].reshape((-1, 1)), opt_result['fun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = np.zeros((3, 1))\n",
    "loss = partial(logistic_loss_theta, X_data, y_data, hyposesis_function)\n",
    "loss_grad = partial(logistic_loss_theta_grad, X_data, y_data, hyposesis_function)\n",
    "theta, best_cost = optimize(theta_init, loss, loss_grad, show=True)\n",
    "print(best_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the decision boundary: two points, draw a line between\n",
    "# Decision boundary occurs when h = 0, or when\n",
    "# theta_0*x1 + theta_1*x2 + theta_2 = 0\n",
    "# y=mx+b is replaced by x2 = (-1/theta1)(theta2 + theta0*x1)\n",
    "\n",
    "line_xs = np.array([np.min(X_data[:,0]), np.max(X_data[:,0])])\n",
    "line_ys = (-1./theta[1])*(theta[2] + theta[0]*line_xs)\n",
    "plot_data(**student_plotting_spec)\n",
    "plt.plot(line_xs, line_ys, 'b-', lw=10, alpha=0.2, label='Decision Boundary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.5  Evaluating logistic regression**\n",
    "\n",
    "After learning the parameters, you can use the model to predict whether a particular student will\n",
    "be admitted. \n",
    "\n",
    "(a) [5 pts] Show that for a student with an Exam 1 score of 45 and an Exam 2 score of 85, you should\n",
    "expect to see an admission probability of 0.776.\n",
    "\n",
    "Another way to evaluate the quality of the parameters we have found is to see how well the\n",
    "learned model predicts on our training set. \n",
    "\n",
    "(b) [10 pts] In this part, your task is to complete the code in\n",
    "*makePrediction*. The predict function will produce “1” or “0” predictions given a dataset and a learned\n",
    "parameter vector $\\theta$. After you have completed the code, the script below will proceed to report the\n",
    "training accuracy of your classifier by computing the percentage of examples it got correct. You\n",
    "should also see a Training Accuracy of 89.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a student with an Exam 1 score of 45 and an Exam 2 score of 85, \n",
    "# you should expect to see an admission probability of 0.776.\n",
    "check_data = np.array([[45., 85., 1]])\n",
    "print(check_data.shape)\n",
    "print(hyposesis_function(check_data, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hyposesis function and broadcast compare operator\n",
    "def predict(x, theta):\n",
    "    raise NotImplementedError('Implement it yourself.')\n",
    "\n",
    "def accuracy(x, y, theta):\n",
    "    raise NotImplementedError('Implement it yourself.')\n",
    "\n",
    "print(accuracy(X_data, y_data, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Regularized logistic regression\n",
    "\n",
    "In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant pass quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly. Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips in *ex2data2.txt*, from which you can build a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Visualizing the data**\n",
    "\n",
    "Similar to the previous parts of this exercise, plotData is used to generate the figure below,\n",
    "where the axes are the two test scores, and the positive (y = 1, accepted) and negative (y = 0,\n",
    "rejected) examples are shown with different markers.\n",
    "\n",
    "The figure below shows that our dataset cannot be separated into positive and negative examples by a\n",
    "straight line. Therefore, a straightforward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_, y_data = read_classification_csv_data('ex2data2.txt')\n",
    "X_data = X_data_ - X_data_.mean(axis=0)[None, :]\n",
    "print(X_data.shape, X_data.min(), X_data.max(), X_data.dtype)\n",
    "print(y_data.shape, y_data.min(), y_data.max(), y_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_plotting_spec = {\n",
    "    'X': X_data,\n",
    "    'y': y_data, \n",
    "    'xlabel': 'Microchip Test 1 Result', \n",
    "    'ylabel': 'Microchip Test 2 Result',\n",
    "    'labels': ['rejected', 'accepted'], \n",
    "    'markers': ['yo', 'k+'],\n",
    "    'figsize': (6, 6)\n",
    "}\n",
    "\n",
    "plot_data(**chip_plotting_spec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Nonlinear feature mapping**\n",
    "\n",
    "One way to fit the data better is to create more features from each data point. In *mapFeature* below, we will map the features into all polynomial terms of $x_1$ and $x_2$ up to the\n",
    "sixth power as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "mapFeature(x) \\ = \\\n",
    "\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "    x_1^2 \\\\\n",
    "    x_1x_2 \\\\\n",
    "    x_2^2 \\\\\n",
    "    x_1^3 \\\\\n",
    "    \\vdots \\\\\n",
    "    x_1x_2^5 \\\\\n",
    "    x_2^6 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "As a result of this mapping, our vector of two features (the scores\n",
    "on two QA tests) has been transformed into a 28-dimensional\n",
    "vector. A logistic regression classifier trained on this\n",
    "higher-dimension feature vector will have a more complex\n",
    "decision boundary and will appear nonlinear when drawn in our\n",
    "2-dimensional plot.\n",
    "While the feature mapping allows us to build a more expressive\n",
    "classifier, it is also more susceptible to overfitting. In the next parts\n",
    "of the exercise, you will implement regularized logistic regression\n",
    "to fit the data and also see for yourself how regularization can help combat the overfitting problem.\n",
    "\n",
    "Either finite dimentional (or even infinite-dimentional, as you would see in the SVM leacture and the corresponding home assingment) feature mappings are usually denoted by $\\Phi$ and therefore our hyposesis is now that the Bernoulli probability of chip matfunctioning might be described as\n",
    "\n",
    "$$ p_i = \\sigma(\\Phi(x_i)^T \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def polynomial_feature_map(X_data, degree=20, show_me_ur_powers=False):\n",
    "    assert len(X_data.shape) == 2\n",
    "    group_size = X_data.shape[1]\n",
    "    assert group_size == 2\n",
    "    # hm.. how to get all ordered pairs (c, d) of non-negative ints \n",
    "    # such that their sum is c + d <= dergee? \n",
    "    # it is eqvivalent to getting all groups of integers (a, b) such that\n",
    "    # 0 <= a <= b <= degree and definintg c = a, d = b - a\n",
    "    # their sum is below degree, both are >= 0 \n",
    "    # then feature_i = (x_0 ^ c) * (x_1 ^ d)\n",
    "    comb_iterator = combinations_with_replacement(range(degree+1), group_size)\n",
    "    not_quite_powers = np.array(list(comb_iterator))\n",
    "    powers_bad_order = not_quite_powers.copy()\n",
    "    powers_bad_order[:, 1] -= not_quite_powers[:, 0]\n",
    "    # let's reoder them so that lower power monomials come first\n",
    "    rising_power_idx = np.argsort(powers_bad_order.sum(axis=1))\n",
    "    powers = powers_bad_order[rising_power_idx]\n",
    "    if show_me_ur_powers is True:\n",
    "        print(powers.T)\n",
    "        print('total power per monomial', powers.sum(axis=1))\n",
    "    X_with_powers = np.power(X_data[:, :, None], powers.T[None])\n",
    "    # tu tu power rangers (with replacement)\n",
    "    X_poly = np.prod(X_with_powers, axis=1)\n",
    "    return X_poly\n",
    "\n",
    "X_pf = polynomial_feature_map(X_data, show_me_ur_powers=True)\n",
    "print(X_pf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Cost function and gradient**\n",
    "\n",
    "Now you will implement code to compute the cost function and gradient for regularized logistic\n",
    "regression. Recall that the regularized cost function in logistic regression is:\n",
    "\n",
    "$j(\\theta) \\ = \\ [ \\ \\frac{1}{m} \\ \\sum_{i=1}^{m} \\ [ \\ -y^{(i)} log(h_\\theta(x^{(i)})) \\ - \\ (1 - y^{(i)})log(1-h_\\theta(x^{(i)})) \\ ] \\ ] \\ + \\frac{\\lambda}{2m} \\sum_{j=2}^{n} \\theta_j^2 $\n",
    "\n",
    "Note that you should not regularize the parameter $\\theta_0$ (Why not? Think about why that would be a bad idea).\n",
    "\n",
    "The gradient of the cost function is a vector where the j element is defined as follows (you should understand how to obtain this expression):\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}} \\ = \\ \\frac{1}{m} \\ \\sum_{i=1}^{m} \\ (h_\\theta(x^{(i)})-y^{(i)}) x_j^{(i)} \\quad \\quad \\quad \\quad \\quad \\quad$ for $\\quad j=0$\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}} \\ = \\ (\\frac{1}{m} \\ \\sum_{i=1}^{m} \\ (h_\\theta(x^{(i)})-y^{(i)}) x_j^{(i)}) + \\frac{\\lambda}{m}\\theta_j \\quad \\quad \\quad$ for $\\quad j \\ge 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.1  Implementing regularized logistic regression**\n",
    "\n",
    "Re-implement computeCost with regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function, default lambda (regularization) 0\n",
    "def logistic_loss_theta_w_reg(x, y, h, theta, lambda_=0.0): \n",
    "    raise NotImplementedError('Implement it yourself.')\n",
    "\n",
    "def logistic_loss_theta_w_reg_grad(x, y, h, theta, lambda_=0.0): \n",
    "    raise NotImplementedError('Implement it yourself.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done, you will call your cost function using the initial value of\n",
    "θ (initialized to all zeros). You should see that the cost is about 0.693. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = np.zeros((X_pf.shape[1], 1))\n",
    "print(logistic_loss_theta_w_reg(X_pf, y_data, hyposesis_function, theta_init))\n",
    "print(logistic_loss_theta_w_reg_grad(X_pf, y_data, hyposesis_function, theta_init))\n",
    "\n",
    "loss = partial(logistic_loss_theta_w_reg, X_pf, y_data, hyposesis_function)\n",
    "loss_grad = partial(logistic_loss_theta_w_reg_grad, X_pf, y_data, hyposesis_function)\n",
    "theta, best_cost = optimize(theta_init, loss, loss_grad, max_iter=10000, print_every=0, show=True)\n",
    "print('best loss', best_cost)\n",
    "print('best acc', accuracy(X_pf, y_data, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Plotting the decision boundary**\n",
    "\n",
    "To help you visualize the model learned by this classifier, we have provided the function\n",
    "*plotBoundary* which plots the (non-linear) decision boundary that separates the\n",
    "positive and negative examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(theta, ax=None):\n",
    "    \"\"\"\n",
    "    Function to plot the decision boundary for arbitrary theta, X, y, lambda value\n",
    "    Inside of this function is feature mapping, and the minimization routine.\n",
    "    It works by making a grid of x1 (\"xvals\") and x2 (\"yvals\") points,\n",
    "    And for each, computing whether the hypothesis classifies that point as\n",
    "    True or False. Then, a contour is drawn with a built-in pyplot function.\n",
    "    \"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    x_range = np.linspace(-1,1.5,50)\n",
    "    y_range = np.linspace(-1,1.5,50)\n",
    "    xx, yy = np.meshgrid(x_range, y_range)\n",
    "    X_fake = np.stack([xx, yy]).reshape(2, -1).T\n",
    "    X_fake_fm = polynomial_feature_map(X_fake)\n",
    "    y_pred_fake = hyposesis_function(X_fake_fm, theta)\n",
    "    return ax.contour( x_range, y_range, y_pred_fake.reshape(50, 50).T, [0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.1 Plot Decision Boundaries**\n",
    "\n",
    "(a) Use *plotBoundary* to obtain four subplots of the decision boundary for the following values of the regularization parameter: $\\lambda \\ = \\ 0, 1, 5, 10$\n",
    "\n",
    "(b) Considering that later components of theta correspond to higher powers of monomials, plot values of theta and commend on effects of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Build a figure showing contours for various values of regularization parameter, lambda\n",
    "\n",
    "np.random.seed(2)\n",
    "train_idx_mask = np.random.rand(X_pf.shape[0]) < 0.3\n",
    "X_pf_train, y_train = X_pf[train_idx_mask], y_data[train_idx_mask]\n",
    "X_pf_test, y_test = X_pf[~train_idx_mask], y_data[~train_idx_mask]\n",
    "print([x.shape for x in (X_pf_train, y_train, X_pf_test, y_test)])\n",
    "\n",
    "def silent_optimize_w_lambda(lambda_):\n",
    "    theta_init = np.zeros((X_pf.shape[1], 1))\n",
    "    data = (X_pf_train, y_train, hyposesis_function)\n",
    "    loss = partial(logistic_loss_theta_w_reg, *data, lambda_=lambda_)\n",
    "    loss_grad = partial(logistic_loss_theta_w_reg_grad, *data, lambda_=lambda_)\n",
    "    optimizer_fn = partial(climin.GradientDescent, step_rate=1e-4, momentum=0.999)\n",
    "    theta, final_loss = optimize(\n",
    "        theta_init, loss, loss_grad, optimizer_fn=optimizer_fn, \n",
    "        max_iter=1000, print_every=0, show=False\n",
    "    )\n",
    "    return theta, final_loss\n",
    "\n",
    "thetas = []\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "# wow, I mutates an object used in the scope of another function (plot_data)\n",
    "# don't do that! it is really hard to debug later\n",
    "chip_plotting_spec['figsize'] = None\n",
    "\n",
    "# you might find following lines useful:\n",
    "#\n",
    "#    cnt_fmt = {0.5: 'Lambda = %d' % lambda_}\n",
    "#    ax.clabel(cnt, inline=1, fontsize=15, fmt=cnt_fmt)\n",
    "# \n",
    "# red dots indicate training samples\n",
    "\n",
    "for id_, lambda_ in enumerate([0, 1, 5, 10]):\n",
    "    ax = plt.subplot(2, 2, id_+1)\n",
    "    raise NotImplementedError('Implement it yourself.')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# (b) Considering that later components of theta correspond to higher powers\n",
    "# of monomials, plot values of theta and commend on effects of regularization\n",
    "ax = None\n",
    "for th_id, theta in enumerate(thetas):\n",
    "    ax = plt.subplot(2, 2, th_id+1, sharey=ax)\n",
    "    raise NotImplementedError('Implement it yourself.')\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
