{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4: Neural Networks\n",
    "\n",
    "This assignment requires a working IPython Notebook installation, which you should already have. If not, please refer to the instructions in Problem Set 2.\n",
    "\n",
    "The programming part is adapted from [Stanford CS231n](http://cs231n.stanford.edu/).\n",
    "\n",
    "Total: 100 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a simple MLP\n",
    "In this problem we will develop a neural network with fully-connected layers, or Multi-Layer Perceptron (MLP). We will use it in classification tasks.\n",
    "\n",
    "In the current directory, you can find a file `mlp.py`, which contains the definition for class `TwoLayerMLP`. As the name suggests, it implements a 2-layer MLP, or MLP with 1 *hidden* layer. You will implement your code in the same file, and call the member functions in this notebook. Below is some initialization. The `autoreload` command makes sure that `mlp.py` is periodically reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mlp import TwoLayerMLP\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize a toy model and some toy data, the task is to classify five 4-d vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model(actv, std=1e-1):\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerMLP(input_size, hidden_size, num_classes, std=std, activation=actv)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "X, y = init_toy_data()\n",
    "print('X = ', X)\n",
    "print()\n",
    "print('y = ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 Forward pass: Sigmoid\n",
    "Our 2-layer MLP uses a softmax output layer (**note**: this means that you don't need to apply a sigmoid on the output) and the multiclass cross-entropy loss to perform classification. \n",
    "\n",
    "Please take a look at method `TwoLayerMLP.loss` in the file `mlp.py`. This function takes in the data and weight parameters, and computes the class scores (aka logits), the loss $L$, and the gradients on the parameters. \n",
    "\n",
    "- Complete the implementation of forward pass (up to the computation of `scores`) for the sigmoid activation: $\\sigma(x)=\\frac{1}{1+exp(-x)}$.\n",
    "\n",
    "**Note 1**: Softmax cross entropy loss involves the [log-sum-exp operation](https://en.wikipedia.org/wiki/LogSumExp). This can result in numerical underflow/overflow. Read about the solution in the link, and try to understand the calculation of `loss` in the code.\n",
    "\n",
    "**Note 2**: You're strongly encouraged to implement in a vectorized way and avoid using slower `for` loops. Note that most numpy functions support vector inputs.\n",
    "\n",
    "Check the correctness of your forward pass below. The difference should be very small (<1e-6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = init_toy_model('sigmoid')\n",
    "loss, _ = net.loss(X, y, reg=0.1)\n",
    "correct_loss = 1.182248\n",
    "print(loss)\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q2.2 Backward pass: Sigmoid\n",
    "- For sigmoid activation, complete the computation of `grads`, which stores the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`.\n",
    "\n",
    "Now debug your backward pass using a numeric gradient check. Again, the differences should be very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "from utils import eval_numerical_gradient\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.1)\n",
    "\n",
    "# these should all be very small\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.1)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e'%(param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q2.3 Train the Sigmoid network\n",
    "To train the network we will use stochastic gradient descent (SGD), implemented in `TwoLayerNet.train`. Then we train a two-layer network on toy data.\n",
    "\n",
    "- Implement the prediction function `TwoLayerNet.predict`, which is called during training to keep track of training and validation accuracy.\n",
    "\n",
    "You should get the final training loss around 0.1, which is good, but not too great for such a toy problem.  One problem is that the gradient magnitude for W1 (the first layer weights) stays small all the time, and the neural net doesn't get much \"learning signals\". This has to do with the saturation problem of the sigmoid activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = init_toy_model('sigmoid', std=1e-1)\n",
    "stats = net.train(X, y, X, y,\n",
    "                  learning_rate=0.5, reg=1e-5,\n",
    "                  num_epochs=100, verbose=False)\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history and gradient magnitudes\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['grad_magnitude_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('||W1||')\n",
    "plt.ylim(0,1)\n",
    "plt.title('Gradient magnitude history (W1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q2.4 Using ReLU activation\n",
    "The Rectified Linear Unit (ReLU) activation is also widely used: $ReLU(x)=max(0,x)$.\n",
    "\n",
    "- Complete the implementation for the ReLU activation (forward and backward) in `mlp.py`.\n",
    "- Train the network with ReLU, and report your final training loss.\n",
    "\n",
    "Make sure you first pass the numerical gradient check on toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = init_toy_model('relu', std=1e-1)\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.1)\n",
    "print('loss = ', loss)  # correct_loss = 1.320973\n",
    "\n",
    "# The differences should all be very small\n",
    "print('checking gradients')\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.1)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e'%(param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's working, let's train the network. Does the net get stronger learning signals (i.e. gradients) this time? Report your final training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = init_toy_model('relu', std=1e-1)\n",
    "stats = net.train(X, y, X, y,\n",
    "                  learning_rate=0.1, reg=1e-5,\n",
    "                  num_epochs=50, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['grad_magnitude_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('||W1||')\n",
    "plt.title('Gradient magnitude history (W1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST data\n",
    "Now that you have implemented a two-layer network that works on toy data, let's try some real data. The MNIST dataset is a standard machine learning benchmark. It consists of 70,000 grayscale handwritten digit images, which we split into 50,000 training, 10,000 validation and 10,000 testing. The images are of size 28x28, which are flattened into 784-d vectors.\n",
    "\n",
    "**Note 1**: the function `get_MNIST_data` requires the `scikit-learn` package. If you previously did anaconda installation to set up your Python environment, you should already have it. Otherwise, you can install it following the instructions here: http://scikit-learn.org/stable/install.html\n",
    "\n",
    "**Note 2**: If you encounter a `HTTP 500` error, that is likely temporary, just try again.\n",
    "\n",
    "**Note 3**: Ensure that the downloaded MNIST file is 55.4MB (smaller file-sizes could indicate an incomplete download - which is possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST\n",
    "from utils import get_MNIST_data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_MNIST_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.5 Train a network on MNIST\n",
    "We will now train a network on MNIST with 64 hidden units in the hidden layer. We train it using SGD, and decrease the learning rate with an exponential rate over time; this is achieved by multiplying the learning rate with a constant factor `learning_rate_decay` (which is less than 1) after each epoch. In effect, we are using a high learning rate initially, which is good for exploring the solution space, and using lower learning rates later to encourage convergence to a local minimum (or [saddle point](http://www.offconvex.org/2016/03/22/saddlepoints/), which may happen more often).\n",
    "\n",
    "- Train your MNIST network with 2 different activation functions: sigmoid and ReLU. \n",
    "\n",
    "We first define some variables and utility functions. The `plot_stats` function plots the histories of gradient magnitude, training loss, and accuracies on the training and validation sets. The `visualize_weights` function visualizes the weights learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized. Both functions help you to diagnose the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 28 * 28\n",
    "hidden_size = 64\n",
    "num_classes = 10\n",
    "\n",
    "# Plot the loss function and train / validation accuracies\n",
    "def plot_stats(stats):\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(stats['grad_magnitude_history'])\n",
    "    plt.title('Gradient magnitude history (W1)')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('||W1||')\n",
    "    plt.ylim(0, np.minimum(100,np.max(stats['grad_magnitude_history'])))\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(stats['loss_history'])\n",
    "    plt.title('Loss history')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(stats['train_acc_history'], label='train') \n",
    "    plt.plot(stats['val_acc_history'], label='val')\n",
    "    plt.title('Classification accuracy history')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Clasification accuracy')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the weights of the network\n",
    "from utils import visualize_grid\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.T.reshape(-1, 28, 28)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q2.5.1 Sigmoid network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_net = TwoLayerMLP(input_size, hidden_size, num_classes, activation='sigmoid', std=1e-1)\n",
    "\n",
    "# Train the network\n",
    "sigmoid_stats = sigmoid_net.train(X_train, y_train, X_val, y_val, \n",
    "                                  num_epochs=20, batch_size=100, \n",
    "                                  learning_rate=1e-3,  learning_rate_decay=0.95, \n",
    "                                  reg=0.5, verbose=True)\n",
    "\n",
    "# Predict on the training set\n",
    "train_acc = (sigmoid_net.predict(X_train) == y_train).mean()\n",
    "print('Sigmoid final training accuracy: ', train_acc)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (sigmoid_net.predict(X_val) == y_val).mean()\n",
    "print('Sigmoid final validation accuracy: ', val_acc)\n",
    "\n",
    "# Predict on the test set\n",
    "test_acc = (sigmoid_net.predict(X_test) == y_test).mean()\n",
    "print('Sigmoid test accuracy: ', test_acc)\n",
    "\n",
    "# show stats and visualizations\n",
    "plot_stats(sigmoid_stats)\n",
    "show_net_weights(sigmoid_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q2.5.2 ReLU network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_net = TwoLayerMLP(input_size, hidden_size, num_classes, activation='relu', std=1e-1)\n",
    "\n",
    "# Train the network\n",
    "relu_stats = relu_net.train(X_train, y_train, X_val, y_val, \n",
    "                            num_epochs=20, batch_size=100,\n",
    "                            learning_rate=1e-3, learning_rate_decay=0.95, \n",
    "                            reg=0.5, verbose=True)\n",
    "# Predict on the training set\n",
    "train_acc = (relu_net.predict(X_train) == y_train).mean()\n",
    "print('ReLU final training accuracy: ', train_acc)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (relu_net.predict(X_val) == y_val).mean()\n",
    "print('ReLU final validation accuracy: ', val_acc)\n",
    "\n",
    "# Predict on the test set\n",
    "test_acc = (relu_net.predict(X_test) == y_test).mean()\n",
    "print('ReLU test accuracy: ', test_acc)\n",
    "\n",
    "# show stats and visualizations\n",
    "plot_stats(relu_stats)\n",
    "show_net_weights(relu_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
